{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ky002\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Packages for data processing and machine learning\n",
    "import numpy as np, pandas as pd, sklearn as sk, torch as th \n",
    "# Packages for webpage crawling\n",
    "import requests as r\n",
    "from bs4 import BeautifulSoup as BS\n",
    "# Packages for nature language processing\n",
    "import spacy, pyinflect\n",
    "from pyinflect import getAllInflections\n",
    "import nltk,emoji,string,re\n",
    "from nltk.corpus import stopwords as sw\n",
    "import gender_guesser.detector as gen\n",
    "# Packages for Twitter API and configuration\n",
    "import tweepy as tw, configparser  \n",
    "# Packages about time\n",
    "import time as t, datetime as dt, rfc3339\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Car          Buy    Drive          Brand Brand Abbreviation  \\\n",
      "0             car     transact      run           Ford                 VW   \n",
      "1            auto      acquire  driving            BMW           Mercedes   \n",
      "2      automobile          buy     rode     Volkswagen               Voho   \n",
      "3             bus      bargain     ride  Mercedes-Benz              Landy   \n",
      "4     convertible     purchase    drove           Audi             Bimmer   \n",
      "5            jeep         shop   riding       Vauxhall                MBZ   \n",
      "6       limousine   purchasing     tour         Toyota                NaN   \n",
      "7         machine    acquiring     trip            Kia                NaN   \n",
      "8           motor       invest    drive        Hyundai                NaN   \n",
      "9          pickup  acquisition   ridden     Land Rover                NaN   \n",
      "10  station wagon          NaN      NaN            NaN                NaN   \n",
      "11          truck          NaN      NaN            NaN                NaN   \n",
      "12            van          NaN      NaN            NaN                NaN   \n",
      "13          wagon          NaN      NaN            NaN                NaN   \n",
      "14        vehicle          NaN      NaN            NaN                NaN   \n",
      "\n",
      "   Brand Model        Car Tool   Car Type  \n",
      "0       fiesta            grip      coupe  \n",
      "1      corolla          bumper  hatchback  \n",
      "2       Series            tyre      sedan  \n",
      "3         polo           brake     sports  \n",
      "4     sportage          bonnet        suv  \n",
      "5       tucson          airbag        NaN  \n",
      "6        corsa     carburettor        NaN  \n",
      "7      A-Class          piston        NaN  \n",
      "8    discovery          engine        NaN  \n",
      "9           A3         battery        NaN  \n",
      "10         NaN       fuel tank        NaN  \n",
      "11         NaN            hood        NaN  \n",
      "12         NaN  steering wheel        NaN  \n",
      "13         NaN     accelerator        NaN  \n",
      "14         NaN        seatbelt        NaN  \n",
      "889 (car OR auto OR automobile OR bus OR convertible OR jeep OR limousine OR machine OR motor OR pickup OR station OR wagon OR truck OR van OR wagon OR vehicle OR Ford OR BMW OR Volkswagen OR Mercedes-Benz OR Audi OR Vauxhall OR Toyota OR Kia OR Hyundai OR Land OR Rover OR VW OR Mercedes OR Voho OR Landy OR Bimmer OR MBZ OR fiesta OR corolla OR Series OR polo OR sportage OR tucson OR corsa OR A-Class OR discovery OR A3 OR grip OR bumper OR tyre OR brake OR bonnet OR airbag OR carburettor OR piston OR engine OR battery OR fuel OR tank OR hood OR steering OR wheel OR accelerator OR seatbelt OR coupe OR hatchback OR sedan OR sports OR suv) (transact OR acquire OR buy OR bargain OR purchase OR shop OR purchasing OR acquiring OR invest OR acquisition OR run OR driving OR rode OR ride OR drove OR riding OR tour OR trip OR drive OR ridden) lang:en place_country:GB -is:nullcast -has:links\n"
     ]
    }
   ],
   "source": [
    "# Reqest the webpages including the synonymsous words or meanings of these topics: car, buy, and drive\n",
    "car_url='https://www.thesaurus.com/browse/car'\n",
    "buy_url='https://www.thesaurus.com/browse/buy'\n",
    "buying_url='https://www.thesaurus.com/browse/buying'\n",
    "drive_url='https://www.thesaurus.com/browse/drive'\n",
    "\n",
    "# Crawling the synonyms of four topics: (car, buy, buying, drive) from Thesurus.com\n",
    "def keyword_extract(url_name,class_name):\n",
    "    page_name=r.get(url_name)\n",
    "    soup_name=BS(page_name.content,'html.parser')\n",
    "    key_soup=soup_name.find('ul', class_=class_name).find_all('a')\n",
    "    list_name=[]\n",
    "    for key in key_soup:\n",
    "        new_key=key['href'][8:].replace('%20',' ')\n",
    "        list_name.append(new_key)\n",
    "    return list_name\n",
    "\n",
    "# I only captured the red-marked words which contain the most closed meanings as the chosen topics\n",
    "car_sym=keyword_extract(car_url,'css-1xohnkh e1ccqdb60')[:14]\n",
    "car_sym.insert(0,'car')\n",
    "buy_sym=keyword_extract(buy_url,'css-wmtunb e1ccqdb60')[:4]\n",
    "buy_sym.insert(0,'buy')\n",
    "buying_sym=keyword_extract(buying_url,'css-1lj4erq e1ccqdb60')[:3]\n",
    "buying_sym.insert(0,'buying')\n",
    "buy_sym=buy_sym+buying_sym\n",
    "drive_sym=keyword_extract(drive_url,'css-n85ndd e1ccqdb60')[:4]\n",
    "drive_sym.insert(0,'drive')\n",
    "\n",
    "# Reqest the webpages including the synonymsous words or meanings of these topics: car, buy, and drive\n",
    "car_url='https://www.thesaurus.com/browse/car'\n",
    "buy_url='https://www.thesaurus.com/browse/buy'\n",
    "buying_url='https://www.thesaurus.com/browse/buying'\n",
    "drive_url='https://www.thesaurus.com/browse/drive'\n",
    "\n",
    "# Crawling the synonyms of four topics: (car, buy, buying, drive) from Thesurus.com\n",
    "def keyword_extract(url_name,class_name):\n",
    "    page_name=r.get(url_name)\n",
    "    soup_name=BS(page_name.content,'html.parser')\n",
    "    key_soup=soup_name.find('ul', class_=class_name).find_all('a')\n",
    "    list_name=[]\n",
    "    for key in key_soup:\n",
    "        new_key=key['href'][8:].replace('%20',' ')\n",
    "        list_name.append(new_key)\n",
    "    return list_name\n",
    "\n",
    "# I only captured the red-marked words which contain the most closed meanings as the chosen topics\n",
    "car_sym=keyword_extract(car_url,'css-1xohnkh e1ccqdb60')[:14]\n",
    "car_sym.insert(0,'car')\n",
    "buy_sym=keyword_extract(buy_url,'css-wmtunb e1ccqdb60')[:4]\n",
    "buy_sym.insert(0,'buy')\n",
    "buying_sym=keyword_extract(buying_url,'css-1lj4erq e1ccqdb60')[:3]\n",
    "buying_sym.insert(0,'buying')\n",
    "buy_sym=buy_sym+buying_sym\n",
    "drive_sym=keyword_extract(drive_url,'css-n85ndd e1ccqdb60')[:4]\n",
    "drive_sym.insert(0,'drive')\n",
    "\n",
    "# Altough I have got thirty words as required keywords for tweet requests, it is recommended to include different tenses or forms of words \n",
    "#, as many Twitter users will use them depending on different contents.\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "buy_str=' '.join(buy_sym)\n",
    "buy_doc=nlp(buy_str)\n",
    "buy_extension_list=[]\n",
    "for num in range(len(buy_doc)):\n",
    "    token = buy_doc[num]\n",
    "    if token.tag_ in ['NN','VB','VBG']:\n",
    "        buy_extension_list.append(token._.inflect('VB',inflect_oov=True))\n",
    "buy_extension_list=[ele for ele in list(set(buy_extension_list+['invest','shop','transact'])) if ele]\n",
    "\n",
    "def extension(sym):\n",
    "    sym_str=' '.join(sym)\n",
    "    sym_token=nlp(sym_str)\n",
    "    extension_list=[]\n",
    "    for num in range(len(sym_token)):\n",
    "        token = sym_token[num]\n",
    "        if token.tag_ in ['NN','VB','VBG']:\n",
    "            if str(token)!=token._.inflect('VBD',inflect_oov=True)[:len(token)]:\n",
    "                extension_list.append(token._.inflect('VBD',inflect_oov=True))              \n",
    "            if str(token)!=token._.inflect('VBG',inflect_oov=True)[:len(token)]:\n",
    "                 extension_list.append(token._.inflect('VBG',inflect_oov=True))\n",
    "            if str(token)!=token._.inflect('VBN',inflect_oov=True)[:len(token)]:\n",
    "                 extension_list.append(token._.inflect('VBN',inflect_oov=True))\n",
    "            if str(token)!=token._.inflect('VBZ',inflect_oov=True)[:len(token)]:\n",
    "                 extension_list.append(token._.inflect('VBZ',inflect_oov=True))\n",
    "    return extension_list\n",
    "\n",
    "buy_sym=list(set(extension(buy_extension_list)+buy_extension_list))\n",
    "buy_sym.append('acquisition')\n",
    "\n",
    "drive_sym=list(set(extension(drive_sym)+drive_sym))\n",
    "\n",
    "# According to car registration records of the UK from 2019 to 2022, the top 10 sales brands were selected as they might be mentioned more frequently than the other brands when talking about automative topics\n",
    "brand_list=['Ford','BMW','Volkswagen','Mercedes-Benz','Audi','Vauxhall','Toyota','Kia','Hyundai','Land Rover']\n",
    "\n",
    "#While searching for Google Trends data, various spellings or expressions that a user could use while posting about a car model were checked.\n",
    "brand_abb=['VW','Mercedes','Voho','Landy','Bimmer','MBZ']\n",
    "\n",
    "# The most popular Car sale Models in the UK, as a same replacement of the brand name: One model for one brand\n",
    "brand_model=['fiesta','corolla','Series','polo','sportage','tucson','corsa','A-Class','discovery','A3']\n",
    "\n",
    "# Name of car types when people are talking a series of car\n",
    "car_type=['coupe','hatchback','sedan','sports','suv']\n",
    "\n",
    "# Names of many important parts of car are also added in as I found that is a part of comments when people value their cars:15 most important words from https://www.collinsdictionary.com/word-lists/car-parts-of-a-car\n",
    "car_tool=['grip','bumper','tyre','brake','bonnet','airbag','carburettor','piston','engine','battery','fuel tank','hood','steering wheel','accelerator','seatbelt']\n",
    "#But there isn't such words like automobiling/automobiled or jeeping/jeeped, so it has to be deleted\n",
    "car_sym.remove('ride')\n",
    "car_sym.append('vehicle')\n",
    "\n",
    "def df_generator(name,list):\n",
    "    return pd.DataFrame({name:list})\n",
    "#Put all the words into a dataframe sorted by different segements.\n",
    "keywords=pd.concat([df_generator('Car',car_sym),\n",
    "                    df_generator('Buy',buy_sym),\n",
    "                    df_generator('Drive',drive_sym),\n",
    "                    df_generator('Brand',brand_list),\n",
    "                    df_generator('Brand Abbreviation',brand_abb),\n",
    "                    df_generator('Brand Model',brand_model), \n",
    "                    df_generator('Car Tool',car_tool),\n",
    "                    df_generator('Car Type',car_type)],\n",
    "                    axis=1)\n",
    "print(keywords)\n",
    "\n",
    "# To limit the search zone that has to contain the elements of car, purchase, drive \n",
    "keylist_of_car=car_sym+brand_list+brand_abb+brand_model+car_tool+car_type\n",
    "keylist_of_buy=buy_sym\n",
    "keylist_of_drive=drive_sym\n",
    "\n",
    "#Build the content of query: element 'car' is compulsory which the other elements are optional\n",
    "query_content='('+' '.join(keylist_of_car).replace(' ',' OR ')+') ('+' '.join(keylist_of_buy+keylist_of_drive).replace(' ',' OR ')+') lang:en place_country:GB -is:nullcast -has:links'\n",
    "print(len(query_content), query_content)\n",
    "# Build and read the config for password safety\n",
    "config=configparser.RawConfigParser()\n",
    "config.read('config.ini')\n",
    "\n",
    "api_key=config['twitter']['api_key']\n",
    "api_key_secret=config['twitter']['api_key_secret']\n",
    "access_token=config['twitter']['access_token']\n",
    "access_token_secret=config['twitter']['access_token_secret']\n",
    "bearer_token=config['twitter']['bearer_token']\n",
    "\n",
    "# Authtication of my Twitter api\n",
    "client = tw.Client(bearer_token,api_key, api_key_secret,access_token, access_token_secret,wait_on_rate_limit=True)\n",
    "\n",
    "# Building the timestrap of each hour\n",
    "def date_range(start_date, end_date):\n",
    "    while start_date <= end_date:\n",
    "        yield start_date\n",
    "        start_date+=dt.timedelta(hours=8)\n",
    "\n",
    "# Set the starting at the first hour of Feburary 1st 2019 and the end time at the same hour of October 1st 2021\n",
    "start_date_1_1 = datetime(2019, 2, 1, 0, 00,00)\n",
    "end_date_1_1 = datetime(2019, 4, 1, 0, 00,00)\n",
    "start_date_1_2 = datetime(2019, 8, 1, 0, 00,00)\n",
    "end_date_1_2 = datetime(2019, 10, 1, 0, 00,00)\n",
    "\n",
    "start_date_2_1 = datetime(2020, 2, 1, 0, 00,00)\n",
    "end_date_2_1 = datetime(2020, 4, 1, 0, 00,00)\n",
    "start_date_2_2 = datetime(2020, 8, 1, 0, 00,00)\n",
    "end_date_2_2 = datetime(2020, 10, 1, 0, 00,00)\n",
    "\n",
    "start_date_3_1 = datetime(2021, 2, 1, 0, 00,00)\n",
    "end_date_3_1 = datetime(2021, 4, 1, 0, 00,00)\n",
    "start_date_3_2 = datetime(2021, 8, 1, 0, 00,00)\n",
    "end_date_3_2 = datetime(2021, 10, 1, 0, 00,00)\n",
    "\n",
    "# remember to change about the time\n",
    "\n",
    "def time_builder(start_date,end_date):\n",
    "    time=[]\n",
    "    for single_date in date_range(start_date, end_date):\n",
    "        time.append(single_date.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "    return time\n",
    "\n",
    "first_time_1=time_builder(start_date_1_1,end_date_1_1)[:-1]+time_builder(start_date_1_2,end_date_1_2)[:-1]\n",
    "second_time_1=time_builder(start_date_1_1,end_date_1_1)[1:]+time_builder(start_date_1_2,end_date_1_2)[1:]\n",
    "\n",
    "first_time_2=time_builder(start_date_2_1,end_date_2_1)[:-1]+time_builder(start_date_2_2,end_date_2_2)[:-1]\n",
    "second_time_2=time_builder(start_date_2_1,end_date_2_1)[1:]+time_builder(start_date_2_2,end_date_2_2)[1:]\n",
    "\n",
    "first_time_3=time_builder(start_date_3_1,end_date_3_1)[:-1]+time_builder(start_date_3_2,end_date_3_2)[:-1]\n",
    "second_time_3=time_builder(start_date_3_1,end_date_3_1)[1:]+time_builder(start_date_3_2,end_date_3_2)[1:]\n",
    "\n",
    "def rfc_time_convetor(time_list):\n",
    "    new_time_list=[]\n",
    "    for single_record in time_list:\n",
    "        datetime_object = datetime.strptime(single_record, \"%Y-%m-%d %H:%M:%S\")\n",
    "        rfc_records=rfc3339.rfc3339(datetime_object)\n",
    "        new_time_list.append(rfc_records)\n",
    "    return new_time_list\n",
    "\n",
    "rfc_first_time=rfc_time_convetor(first_time_1)+rfc_time_convetor(first_time_2)+rfc_time_convetor(first_time_3)\n",
    "rfc_second_time=rfc_time_convetor(second_time_1)+rfc_time_convetor(second_time_2)+rfc_time_convetor(second_time_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve all the required data from Twitter API from each eight hours per day of three years\n",
    "for start_time, end_time in zip(rfc_first_time,rfc_second_time):\n",
    "    tweet_info_small_list=[]\n",
    "    tweets=client.search_all_tweets(\n",
    "                            query_content,                            \n",
    "                            end_time=end_time,       \n",
    "                            start_time=start_time,\n",
    "                            tweet_fields = [\"created_at\", \"text\", \"lang\"],\n",
    "                            user_fields = ['name', 'username', \"location\"],\n",
    "                            sort_order=['relevancy'],\n",
    "                            expansions='author_id',\n",
    "                            max_results=100)\n",
    "    for tweet,user in zip(tweets.data,tweets.includes['users']):\n",
    "        tweet_info = {\n",
    "        'created_at': tweet.created_at,\n",
    "        'text': tweet.text,\n",
    "        'source': tweet.source,\n",
    "        'name': user.name,\n",
    "        'username': user.username,\n",
    "        'location': user.location}\n",
    "        tweet_info_small_list.append(tweet_info)\n",
    "    tweets_datasource = pd.DataFrame(tweet_info_small_list)\n",
    "    tweets_datasource.to_csv('C:/home2/hzhx55/Dissertation/TwData_300.csv',sep=',', mode='a',encoding='utf_8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get rid of the empty lines and sort all the data by the time it happened\n",
    "raw_data=pd.read_csv('TwData_300.csv',sep=',',header=0,encoding='utf_8')\n",
    "raw_data=raw_data.drop(columns=['Unnamed: 0','source'])\n",
    "raw_data=raw_data[raw_data['text']!='text'].sort_values(by='created_at').reset_index(drop=True)\n",
    "\n",
    "def treatment_marker(row):\n",
    "    if row.created_at<'2019-10-01 00:00:00+00:00':\n",
    "        answer='before covid'\n",
    "    else:\n",
    "        answer='after covid'\n",
    "    return answer\n",
    "\n",
    "raw_data['treatment/control']=raw_data.apply(treatment_marker,axis=1)\n",
    "\n",
    "#Remove all the emoji used in the text\n",
    "def emoji_free_text(text):\n",
    "    pattern = re.compile(pattern = \"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FC\"  # transport & map symbols\n",
    "        u\"\\U0001F1E6-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u'\\U00010000-\\U0010ffff'\n",
    "        u\"\\u2640-\\u2642\"\n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\u3030\"\n",
    "        u\"\\ufe0f\"\n",
    "        u\"\\u200a-\\u200f\"\n",
    "        \"]+\", \n",
    "        flags = re.UNICODE)\n",
    "    return pattern.sub(r'',text)\n",
    "    \n",
    "#Remove all the puntutation and number used in the text\n",
    "def puntuation_free_text(text):\n",
    "    tab=str.maketrans(dict.fromkeys(string.punctuation))\n",
    "    pure_text=text.translate(tab)\n",
    "    pure_text=''.join([let for let in pure_text if not let.isdigit()])\n",
    "    return pure_text\n",
    "\n",
    "def remove_meaningless_name(text):\n",
    "    exclude_words_of_name=sw.words('english')\n",
    "    new_name=[]\n",
    "    for word in text.split():\n",
    "        if word not in exclude_words_of_name:\n",
    "            new_name.append(word)\n",
    "    return ' '.join(new_name)\n",
    "\n",
    "# Remove all the space used at the start of sentence, lower the case, and only need the first name of users\n",
    "def remove_space(text):\n",
    "    if len(text)==0:\n",
    "        text='unknown'\n",
    "    else:\n",
    "        if text[0]==' ':\n",
    "            text=text.lower()[1:]\n",
    "        else:\n",
    "            text=text.lower()\n",
    "    return text\n",
    "\n",
    "def remove_alpha(text):\n",
    "    for word in text.split():\n",
    "        if len(word)>1:\n",
    "            pattern=re.compile(word)\n",
    "            poistion=[match.start() for match in pattern.finditer(text)][0]\n",
    "            text=text[poistion:]\n",
    "            break\n",
    "    return text\n",
    "\n",
    "gender=['unknown']*len(raw_data.name)\n",
    "fitered_name_list=[]\n",
    "gender_list=[]\n",
    "\n",
    "for name,sex,username in zip(raw_data.name,gender,raw_data.username):\n",
    "    name=remove_alpha(remove_space(remove_meaningless_name(puntuation_free_text(emoji_free_text(name)))))\n",
    "    if (name[0:3]=='sir'and len(name.split()[0])==3):\n",
    "        name=name[4:]\n",
    "        sex='male'\n",
    "    elif (name[0:2]=='mr'and len(name.split()[0])==2):\n",
    "        name=name[3:]\n",
    "        sex='male'\n",
    "    elif (name[0:2]=='dr'and len(name.split()[0])==2):\n",
    "        name=name[3:]\n",
    "        sex='male'\n",
    "    elif (name[0:4]=='prof' and len(name.split()[0])==4):\n",
    "        name=name[5:]\n",
    "        sex='male'\n",
    "    elif (name[0:4]=='miss' and len(name.split()[0])==4):\n",
    "        name=name[5:]\n",
    "        sex='female'\n",
    "    elif (name[0:5]=='madam'and len(name.split()[0])==5):\n",
    "        name=name[6:]\n",
    "        sex='female'\n",
    "    elif (name[0:2]=='ms'and len(name.split()[0])==2):\n",
    "        name=name[3:]\n",
    "        sex='female'\n",
    "    elif (name[0:3]=='mrs'and len(name.split()[0])==3):\n",
    "        name=name[4:]\n",
    "        sex='female'\n",
    "    elif (name[0:2]=='mx'and len(name.split()[0])==2):\n",
    "       name=name[2:]\n",
    "       sex='unknown'\n",
    "    if len(name.split())==0:\n",
    "        name='unknown'\n",
    "    else:\n",
    "        first_name = ''.join([word for word in name.split()[0]])\n",
    "    if first_name!='unknown':\n",
    "        first_name=first_name.capitalize()\n",
    "    if len(first_name)<=2:\n",
    "        first_name=remove_alpha(remove_meaningless_name(puntuation_free_text(emoji_free_text(username)))).capitalize()\n",
    "    if len(re.findall('([A-Z][a-z]+)', first_name))!=0:\n",
    "        first_name=re.findall('([A-Z][a-z]+)', first_name)[0]   \n",
    "    if len(first_name)==1:\n",
    "        first_name='unknown'\n",
    "        pfint(firs)\n",
    "    fitered_name_list.append(first_name)\n",
    "    gender_list.append(sex)\n",
    "\n",
    "raw_data['first name']=pd.DataFrame(fitered_name_list)\n",
    "raw_data['gender']=pd.DataFrame(gender_list)\n",
    "known_name=raw_data['first name'].loc[raw_data['gender'] =='unknown']\n",
    "detector=gen.Detector()\n",
    "raw_data['predicted gender']=raw_data['first name'].apply(lambda a: detector.get_gender(a))\n",
    "raw_data['gender'].loc[(raw_data['gender']=='unknown')] = raw_data['predicted gender']\n",
    "\n",
    "raw_data.loc[(raw_data['gender']=='mostly_female'),'gender']='female'\n",
    "raw_data.loc[(raw_data['gender']=='mostly_male'),'gender']='male'\n",
    "raw_data.loc[(raw_data['gender']=='andy'),'gender']='unknown'\n",
    "raw_data=raw_data.drop(columns=['predicted gender'])\n",
    "\n",
    "#Remove all the contact signs and people names mentioned inside the text\n",
    "def free_contact(text):\n",
    "    pattern = re.compile(\"@+\")\n",
    "    if len(re.findall(pattern,text))!=0:\n",
    "        pattern = re.compile(\"@+\")\n",
    "        times=len([match.span() for match in pattern.finditer(text)])\n",
    "        time=0\n",
    "        while time<times:\n",
    "            poistion=[match.start() for match in pattern.finditer(text)][0]\n",
    "            text=text[:poistion]+text[poistion+len(text[poistion:].split()[0]):]\n",
    "            text=text.replace(\"  \", \" \")\n",
    "            time+=1\n",
    "        else:\n",
    "            text=text\n",
    "    return text\n",
    "\n",
    "#Restore all emoji into readable meanings\n",
    "def emoji_lemon(text):\n",
    "    return emoji.demojize(text, delimiters=(\"\", \"\")) \n",
    "\n",
    "def single_quote_transfer(text):\n",
    "    text=re.sub(\"’ve\",\" have\",text)\n",
    "    text=re.sub(\"’d\",\" would like to\",text)\n",
    "    text=re.sub(\"n’t\",\" not\",text)\n",
    "    text=re.sub(\"’m\",\" am\",text)\n",
    "    text=re.sub(\"’s\",\" is\",text)\n",
    "    return text\n",
    "\n",
    "new_processed_text=[]\n",
    "for text in raw_data.text:\n",
    "    pre_processed_text=remove_space(remove_meaningless_name(single_quote_transfer(puntuation_free_text(emoji_lemon(free_contact(text))))))\n",
    "    new_processed_text.append(pre_processed_text)\n",
    "\n",
    "raw_data.insert(loc=2, column='new_processed_text', value=new_processed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(len(raw_data.loc[raw_data['first name']=='unknown']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>text</th>\n",
       "      <th>new_processed_text</th>\n",
       "      <th>name</th>\n",
       "      <th>username</th>\n",
       "      <th>location</th>\n",
       "      <th>treatment/control</th>\n",
       "      <th>first name</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [created_at, text, new_processed_text, name, username, location, treatment/control, first name, gender]\n",
       "Index: []"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.loc[raw_data['first name']=='unknown'][0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 Sea\n"
     ]
    }
   ],
   "source": [
    "s='Sea Witch'\n",
    "print(len(re.findall('([A-Z][a-z]+)', s)),re.findall('([A-Z][a-z]+)', s)[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c'"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i=0\n",
    "if i<1:\n",
    "    i='a'\n",
    "if i!='b':\n",
    "    i='c'\n",
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "46c53dfe207e1bf458957234df624eec1401b1cc104da5f2e9e6134dc26f11c7"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

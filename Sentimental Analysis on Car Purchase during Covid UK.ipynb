{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ky002\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Packages for data processing and machine learning\n",
    "import numpy as np, pandas as pd, sklearn as sk, torch as th \n",
    "# Packages for webpage crawling\n",
    "import requests as r\n",
    "from bs4 import BeautifulSoup as BS\n",
    "# Packages for nature language processing\n",
    "import spacy, pyinflect\n",
    "from pyinflect import getAllInflections\n",
    "import nltk,emoji,string,re\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords as sw\n",
    "from nltk.util import ngrams as ng\n",
    "from nltk.tokenize import word_tokenize as tk\n",
    "from nltk.stem import WordNetLemmatizer as wn\n",
    "import gender_guesser.detector as gen\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from textblob import TextBlob\n",
    "# Packages for Twitter API and configuration\n",
    "import tweepy as tw, configparser  \n",
    "# Packages about time\n",
    "import time as t, datetime as dt, rfc3339\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Car          Buy    Drive          Brand Brand Abbreviation  \\\n",
      "0             car   purchasing    drove           Ford                 VW   \n",
      "1            auto     purchase     rode            BMW           Mercedes   \n",
      "2      automobile      bargain  driving     Volkswagen               Voho   \n",
      "3             bus          buy      run  Mercedes-Benz              Landy   \n",
      "4     convertible      acquire   ridden           Audi             Bimmer   \n",
      "5            jeep     transact     trip       Vauxhall                MBZ   \n",
      "6       limousine       invest    drive         Toyota                NaN   \n",
      "7         machine         shop     tour            Kia                NaN   \n",
      "8           motor    acquiring     ride        Hyundai                NaN   \n",
      "9          pickup  acquisition   riding     Land Rover                NaN   \n",
      "10  station wagon          NaN      NaN            NaN                NaN   \n",
      "11          truck          NaN      NaN            NaN                NaN   \n",
      "12            van          NaN      NaN            NaN                NaN   \n",
      "13          wagon          NaN      NaN            NaN                NaN   \n",
      "14        vehicle          NaN      NaN            NaN                NaN   \n",
      "\n",
      "   Brand Model        Car Tool   Car Type  \n",
      "0       fiesta            grip      coupe  \n",
      "1      corolla          bumper  hatchback  \n",
      "2       Series            tyre      sedan  \n",
      "3         polo           brake     sports  \n",
      "4     sportage          bonnet        suv  \n",
      "5       tucson          airbag        NaN  \n",
      "6        corsa     carburettor        NaN  \n",
      "7      A-Class          piston        NaN  \n",
      "8    discovery          engine        NaN  \n",
      "9           A3         battery        NaN  \n",
      "10         NaN       fuel tank        NaN  \n",
      "11         NaN            hood        NaN  \n",
      "12         NaN  steering wheel        NaN  \n",
      "13         NaN     accelerator        NaN  \n",
      "14         NaN        seatbelt        NaN  \n",
      "889 (car OR auto OR automobile OR bus OR convertible OR jeep OR limousine OR machine OR motor OR pickup OR station OR wagon OR truck OR van OR wagon OR vehicle OR Ford OR BMW OR Volkswagen OR Mercedes-Benz OR Audi OR Vauxhall OR Toyota OR Kia OR Hyundai OR Land OR Rover OR VW OR Mercedes OR Voho OR Landy OR Bimmer OR MBZ OR fiesta OR corolla OR Series OR polo OR sportage OR tucson OR corsa OR A-Class OR discovery OR A3 OR grip OR bumper OR tyre OR brake OR bonnet OR airbag OR carburettor OR piston OR engine OR battery OR fuel OR tank OR hood OR steering OR wheel OR accelerator OR seatbelt OR coupe OR hatchback OR sedan OR sports OR suv) (purchasing OR purchase OR bargain OR buy OR acquire OR transact OR invest OR shop OR acquiring OR acquisition OR drove OR rode OR driving OR run OR ridden OR trip OR drive OR tour OR ride OR riding) lang:en place_country:GB -is:nullcast -has:links\n"
     ]
    }
   ],
   "source": [
    "# Reqest the webpages including the synonymsous words or meanings of these topics: car, buy, and drive\n",
    "car_url='https://www.thesaurus.com/browse/car'\n",
    "buy_url='https://www.thesaurus.com/browse/buy'\n",
    "buying_url='https://www.thesaurus.com/browse/buying'\n",
    "drive_url='https://www.thesaurus.com/browse/drive'\n",
    "\n",
    "# Crawling the synonyms of four topics: (car, buy, buying, drive) from Thesurus.com\n",
    "def keywordExtract(url_name,class_name):\n",
    "    page_name=r.get(url_name)\n",
    "    soup_name=BS(page_name.content,'html.parser')\n",
    "    key_soup=soup_name.find('ul', class_=class_name).find_all('a')\n",
    "    list_name=[]\n",
    "    for key in key_soup:\n",
    "        new_key=key['href'][8:].replace('%20',' ')\n",
    "        list_name.append(new_key)\n",
    "    return list_name\n",
    "\n",
    "# I only captured the red-marked words which contain the most closed meanings as the chosen topics\n",
    "car_sym=keywordExtract(car_url,'css-1xohnkh e1ccqdb60')[:14]\n",
    "car_sym.insert(0,'car')\n",
    "buy_sym=keywordExtract(buy_url,'css-wmtunb e1ccqdb60')[:4]\n",
    "buy_sym.insert(0,'buy')\n",
    "buying_sym=keywordExtract(buying_url,'css-1lj4erq e1ccqdb60')[:3]\n",
    "buying_sym.insert(0,'buying')\n",
    "buy_sym=buy_sym+buying_sym\n",
    "drive_sym=keywordExtract(drive_url,'css-n85ndd e1ccqdb60')[:4]\n",
    "drive_sym.insert(0,'drive')\n",
    "\n",
    "# Altough I have got thirty words as required keywords for tweet requests, it is recommended to include different tenses or forms of words \n",
    "#, as many Twitter users will use them depending on different contents.\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "buy_str=' '.join(buy_sym)\n",
    "buy_doc=nlp(buy_str)\n",
    "buy_extension_list=[]\n",
    "for num in range(len(buy_doc)):\n",
    "    token = buy_doc[num]\n",
    "    if token.tag_ in ['NN','VB','VBG']:\n",
    "        buy_extension_list.append(token._.inflect('VB',inflect_oov=True))\n",
    "buy_extension_list=[ele for ele in list(set(buy_extension_list+['invest','shop','transact'])) if ele]\n",
    "\n",
    "def extension(sym):\n",
    "    sym_str=' '.join(sym)\n",
    "    sym_token=nlp(sym_str)\n",
    "    extension_list=[]\n",
    "    for num in range(len(sym_token)):\n",
    "        token = sym_token[num]\n",
    "        if token.tag_ in ['NN','VB','VBG']:\n",
    "            if str(token)!=token._.inflect('VBD',inflect_oov=True)[:len(token)]:\n",
    "                extension_list.append(token._.inflect('VBD',inflect_oov=True))              \n",
    "            if str(token)!=token._.inflect('VBG',inflect_oov=True)[:len(token)]:\n",
    "                 extension_list.append(token._.inflect('VBG',inflect_oov=True))\n",
    "            if str(token)!=token._.inflect('VBN',inflect_oov=True)[:len(token)]:\n",
    "                 extension_list.append(token._.inflect('VBN',inflect_oov=True))\n",
    "            if str(token)!=token._.inflect('VBZ',inflect_oov=True)[:len(token)]:\n",
    "                 extension_list.append(token._.inflect('VBZ',inflect_oov=True))\n",
    "    return extension_list\n",
    "\n",
    "buy_sym=list(set(extension(buy_extension_list)+buy_extension_list))\n",
    "buy_sym.append('acquisition')\n",
    "\n",
    "drive_sym=list(set(extension(drive_sym)+drive_sym))\n",
    "\n",
    "# According to car registration records of the UK from 2019 to 2022, the top 10 sales brands were selected as they might be mentioned more frequently than the other brands when talking about automative topics\n",
    "brand_list=['Ford','BMW','Volkswagen','Mercedes-Benz','Audi','Vauxhall','Toyota','Kia','Hyundai','Land Rover']\n",
    "\n",
    "#While searching for Google Trends data, various spellings or expressions that a user could use while posting about a car model were checked.\n",
    "brand_abb=['VW','Mercedes','Voho','Landy','Bimmer','MBZ']\n",
    "\n",
    "# The most popular Car sale Models in the UK, as a same replacement of the brand name: One model for one brand\n",
    "brand_model=['fiesta','corolla','Series','polo','sportage','tucson','corsa','A-Class','discovery','A3']\n",
    "\n",
    "# Name of car types when people are talking a series of car\n",
    "car_type=['coupe','hatchback','sedan','sports','suv']\n",
    "\n",
    "# Names of many important parts of car are also added in as I found that is a part of comments when people value their cars:15 most important words from https://www.collinsdictionary.com/word-lists/car-parts-of-a-car\n",
    "car_tool=['grip','bumper','tyre','brake','bonnet','airbag','carburettor','piston','engine','battery','fuel tank','hood','steering wheel','accelerator','seatbelt']\n",
    "#But there isn't such words like automobiling/automobiled or jeeping/jeeped, so it has to be deleted\n",
    "car_sym.remove('ride')\n",
    "car_sym.append('vehicle')\n",
    "\n",
    "def dfGenerator(name,list):\n",
    "    return pd.DataFrame({name:list})\n",
    "#Put all the words into a dataframe sorted by different segements.\n",
    "keywords=pd.concat([dfGenerator('Car',car_sym),\n",
    "                    dfGenerator('Buy',buy_sym),\n",
    "                    dfGenerator('Drive',drive_sym),\n",
    "                    dfGenerator('Brand',brand_list),\n",
    "                    dfGenerator('Brand Abbreviation',brand_abb),\n",
    "                    dfGenerator('Brand Model',brand_model), \n",
    "                    dfGenerator('Car Tool',car_tool),\n",
    "                    dfGenerator('Car Type',car_type)],\n",
    "                    axis=1)\n",
    "print(keywords)\n",
    "\n",
    "# To limit the search zone that has to contain the elements of car, purchase, drive \n",
    "keylist_of_car=car_sym+brand_list+brand_abb+brand_model+car_tool+car_type\n",
    "keylist_of_buy=buy_sym\n",
    "keylist_of_drive=drive_sym\n",
    "\n",
    "#Build the content of query: element 'car' is compulsory which the other elements are optional\n",
    "query_content='('+' '.join(keylist_of_car).replace(' ',' OR ')+') ('+' '.join(keylist_of_buy+keylist_of_drive).replace(' ',' OR ')+') lang:en place_country:GB -is:nullcast -has:links'\n",
    "print(len(query_content), query_content)\n",
    "# Build and read the config for password safety\n",
    "config=configparser.RawConfigParser()\n",
    "config.read('config.ini')\n",
    "\n",
    "api_key=config['twitter']['api_key']\n",
    "api_key_secret=config['twitter']['api_key_secret']\n",
    "access_token=config['twitter']['access_token']\n",
    "access_token_secret=config['twitter']['access_token_secret']\n",
    "bearer_token=config['twitter']['bearer_token']\n",
    "\n",
    "# Authtication of my Twitter api\n",
    "client = tw.Client(bearer_token,api_key, api_key_secret,access_token, access_token_secret,wait_on_rate_limit=True)\n",
    "\n",
    "# Building the timestrap of each hour\n",
    "def dateRange(start_date, end_date):\n",
    "    while start_date <= end_date:\n",
    "        yield start_date\n",
    "        start_date+=dt.timedelta(hours=8)\n",
    "\n",
    "# Set the starting at the first hour of Feburary 1st 2019 and the end time at the same hour of October 1st 2021\n",
    "start_date_1_1 = datetime(2019, 2, 1, 0, 00,00)\n",
    "end_date_1_1 = datetime(2019, 4, 1, 0, 00,00)\n",
    "start_date_1_2 = datetime(2019, 8, 1, 0, 00,00)\n",
    "end_date_1_2 = datetime(2019, 10, 1, 0, 00,00)\n",
    "\n",
    "start_date_2_1 = datetime(2020, 2, 1, 0, 00,00)\n",
    "end_date_2_1 = datetime(2020, 4, 1, 0, 00,00)\n",
    "start_date_2_2 = datetime(2020, 8, 1, 0, 00,00)\n",
    "end_date_2_2 = datetime(2020, 10, 1, 0, 00,00)\n",
    "\n",
    "start_date_3_1 = datetime(2021, 2, 1, 0, 00,00)\n",
    "end_date_3_1 = datetime(2021, 4, 1, 0, 00,00)\n",
    "start_date_3_2 = datetime(2021, 8, 1, 0, 00,00)\n",
    "end_date_3_2 = datetime(2021, 10, 1, 0, 00,00)\n",
    "\n",
    "# remember to change about the time\n",
    "\n",
    "def time_builder(start_date,end_date):\n",
    "    time=[]\n",
    "    for single_date in dateRange(start_date, end_date):\n",
    "        time.append(single_date.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "    return time\n",
    "\n",
    "first_time_1=time_builder(start_date_1_1,end_date_1_1)[:-1]+time_builder(start_date_1_2,end_date_1_2)[:-1]\n",
    "second_time_1=time_builder(start_date_1_1,end_date_1_1)[1:]+time_builder(start_date_1_2,end_date_1_2)[1:]\n",
    "\n",
    "first_time_2=time_builder(start_date_2_1,end_date_2_1)[:-1]+time_builder(start_date_2_2,end_date_2_2)[:-1]\n",
    "second_time_2=time_builder(start_date_2_1,end_date_2_1)[1:]+time_builder(start_date_2_2,end_date_2_2)[1:]\n",
    "\n",
    "first_time_3=time_builder(start_date_3_1,end_date_3_1)[:-1]+time_builder(start_date_3_2,end_date_3_2)[:-1]\n",
    "second_time_3=time_builder(start_date_3_1,end_date_3_1)[1:]+time_builder(start_date_3_2,end_date_3_2)[1:]\n",
    "\n",
    "def rfcTimeConvetor(time_list):\n",
    "    new_time_list=[]\n",
    "    for single_record in time_list:\n",
    "        datetime_object = datetime.strptime(single_record, \"%Y-%m-%d %H:%M:%S\")\n",
    "        rfc_records=rfc3339.rfc3339(datetime_object)\n",
    "        new_time_list.append(rfc_records)\n",
    "    return new_time_list\n",
    "\n",
    "rfc_first_time=rfcTimeConvetor(first_time_1)+rfcTimeConvetor(first_time_2)+rfcTimeConvetor(first_time_3)\n",
    "rfc_second_time=rfcTimeConvetor(second_time_1)+rfcTimeConvetor(second_time_2)+rfcTimeConvetor(second_time_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Retrieve all the required data from Twitter API from each eight hours per day of three years\n",
    "# for start_time, end_time in zip(rfc_first_time,rfc_second_time):\n",
    "#     tweet_info_small_list=[]\n",
    "#     tweets=client.search_all_tweets(\n",
    "#                             query_content,                            \n",
    "#                             end_time=end_time,       \n",
    "#                             start_time=start_time,\n",
    "#                             tweet_fields = [\"created_at\", \"text\", \"lang\"],\n",
    "#                             user_fields = ['name', 'username', \"location\"],\n",
    "#                             sort_order=['relevancy'],\n",
    "#                             expansions='author_id',\n",
    "#                             max_results=100)\n",
    "#     for tweet,user in zip(tweets.data,tweets.includes['users']):\n",
    "#         tweet_info = {\n",
    "#         'created_at': tweet.created_at,\n",
    "#         'text': tweet.text,\n",
    "#         'source': tweet.source,\n",
    "#         'name': user.name,\n",
    "#         'username': user.username,\n",
    "#         'location': user.location}\n",
    "#         tweet_info_small_list.append(tweet_info)\n",
    "#     tweets_datasource = pd.DataFrame(tweet_info_small_list)\n",
    "#     tweets_datasource.to_csv('C:/home2/hzhx55/Dissertation/TwData_300.csv',sep=',', mode='a',encoding='utf_8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capacity of the dataset is: 35823\n"
     ]
    }
   ],
   "source": [
    "#Get rid of the empty lines and sort all the data by the time it happened\n",
    "raw_data=pd.read_csv('TwData_300.csv',sep=',',header=0,encoding='utf_8')\n",
    "raw_data=raw_data.drop(columns=['Unnamed: 0','source'])\n",
    "raw_data=raw_data[raw_data['text']!='text'].sort_values(by='created at').reset_index(drop=True)\n",
    "\n",
    "def treatmentMarker(row):\n",
    "    if row['created at']<'2019-10-01 00:00:00+00:00':\n",
    "        answer='before covid'\n",
    "    else:\n",
    "        answer='after covid'\n",
    "    return answer\n",
    "\n",
    "raw_data['treatment/control']=raw_data.apply(treatmentMarker,axis=1)\n",
    "\n",
    "#Remove all the emoji used in the text\n",
    "def emojiFreeText(text):\n",
    "    pattern = re.compile(pattern = \"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FC\"  # transport & map symbols\n",
    "        u\"\\U0001F1E6-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u'\\U00010000-\\U0010ffff'\n",
    "        u\"\\u2640-\\u2642\"\n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\u3030\"\n",
    "        u\"\\ufe0f\"\n",
    "        u\"\\u200a-\\u200f\"\n",
    "        \"]+\", \n",
    "        flags = re.UNICODE)\n",
    "    return pattern.sub(r'',text)\n",
    "    \n",
    "#Remove all the puntutation and number used in the text\n",
    "def puntuationFreeText(text):\n",
    "    tab=str.maketrans(dict.fromkeys(string.punctuation))\n",
    "    pure_text=text.translate(tab)\n",
    "    pure_text=''.join([let for let in pure_text if not let.isdigit()])\n",
    "    return pure_text\n",
    "\n",
    "def removeMeaninglessName(text):\n",
    "    exclude_words_of_name=sw.words('english')\n",
    "    new_name=[]\n",
    "    for word in text.split():\n",
    "        if word not in exclude_words_of_name:\n",
    "            new_name.append(word)\n",
    "    return ' '.join(new_name)\n",
    "\n",
    "# Remove all the space used at the start of sentence, lower the case, and only need the first name of users\n",
    "def removeSpace(text):\n",
    "    if len(text)==0:\n",
    "        text='unknown'\n",
    "    else:\n",
    "        if text[0]==' ':\n",
    "            text=text.lower()[1:]\n",
    "        else:\n",
    "            text=text.lower()\n",
    "    return text\n",
    "\n",
    "def removeAlpha(text):\n",
    "    for word in text.split():\n",
    "        if len(word)>1:\n",
    "            pattern=re.compile(word)\n",
    "            poistion=[match.start() for match in pattern.finditer(text)][0]\n",
    "            text=text[poistion:]\n",
    "            break\n",
    "    return text\n",
    "\n",
    "gender=['unknown']*len(raw_data.name)\n",
    "fitered_name_list=[]\n",
    "gender_list=[]\n",
    "\n",
    "for nam,sex,username in zip(raw_data.name,gender,raw_data.username):\n",
    "    name=removeAlpha(removeSpace(removeMeaninglessName(puntuationFreeText(emojiFreeText(nam)))))\n",
    "    if (name[0:3]=='sir'and len(name.split()[0])==3):\n",
    "        name=name[4:]\n",
    "        sex='male'\n",
    "    elif (name[0:2]=='mr'and len(name.split()[0])==2):\n",
    "        name=name[3:]\n",
    "        sex='male'\n",
    "    elif (name[0:2]=='dr'and len(name.split()[0])==2):\n",
    "        name=name[3:]\n",
    "        sex='male'\n",
    "    elif (name[0:4]=='prof' and len(name.split()[0])==4):\n",
    "        name=name[5:]\n",
    "        sex='male'\n",
    "    elif (name[0:4]=='miss' and len(name.split()[0])==4):\n",
    "        name=name[5:]\n",
    "        sex='female'\n",
    "    elif (name[0:5]=='madam'and len(name.split()[0])==5):\n",
    "        name=name[6:]\n",
    "        sex='female'\n",
    "    elif (name[0:4]=='lady'and len(name.split()[0])==4):\n",
    "        name=name[5:]\n",
    "        sex='female'\n",
    "    elif (name[0:2]=='ms'and len(name.split()[0])==2):\n",
    "        name=name[3:]\n",
    "        sex='female'\n",
    "    elif (name[0:3]=='mrs'and len(name.split()[0])==3):\n",
    "        name=name[4:]\n",
    "        sex='female'\n",
    "    elif (name[0:5]=='nurse'):\n",
    "        name=name[6:]\n",
    "        sex='female'\n",
    "    elif (name[0:2]=='mx'and len(name.split()[0])==2):\n",
    "       name=name[3:]\n",
    "       sex='unknown'\n",
    "    if len(name.split())==0:\n",
    "        name='unknown'\n",
    "    else:\n",
    "        first_name = ''.join([word for word in name.split()[0] if word!='unknown'])\n",
    "        first_name =''.join([word[0].upper() + word[1:] for word in first_name.split()])\n",
    "        if len(re.findall('([A-Z][a-z]+)', first_name))!=0:\n",
    "            first_name=re.findall('([A-Z][a-z]+)', first_name)[0]\n",
    "        else:\n",
    "            first_name=removeAlpha(removeSpace(puntuationFreeText(username)))\n",
    "            first_name =''.join([word[0].upper() + word[1:] for word in first_name.split()])\n",
    "            first_name=re.findall('([A-Z][a-z]+)', first_name)[0]\n",
    "            if len(first_name)<=2 or len(first_name)>10:\n",
    "                first_name=max(first_name.split(), key=len)\n",
    "    if len(first_name)>10:\n",
    "        origin_name=removeAlpha(removeMeaninglessName(puntuationFreeText(emojiFreeText(nam))))\n",
    "        if len(re.findall('([A-Z][a-z]+)', origin_name))!=0:\n",
    "            first_name=re.findall('([A-Z][a-z]+)', origin_name)[0]\n",
    "    fitered_name_list.append(first_name)\n",
    "    gender_list.append(sex)\n",
    "\n",
    "raw_data['first name']=pd.DataFrame(fitered_name_list)\n",
    "raw_data['gender']=pd.DataFrame(gender_list)\n",
    "known_name=raw_data['first name'].loc[raw_data['gender'] =='unknown']\n",
    "detector=gen.Detector()\n",
    "raw_data['predicted gender']=raw_data['first name'].apply(lambda a: detector.get_gender(a))\n",
    "raw_data['gender'].loc[(raw_data['gender']=='unknown')] = raw_data['predicted gender']\n",
    "\n",
    "raw_data.loc[(raw_data['gender']=='mostly_female'),'gender']='female'\n",
    "raw_data.loc[(raw_data['gender']=='mostly_male'),'gender']='male'\n",
    "raw_data.loc[(raw_data['gender']=='andy'),'gender']='unknown'\n",
    "raw_data=raw_data.drop(columns=['predicted gender'])\n",
    "\n",
    "#Assume the people who live in the city have better purchasing power than people who live outside the city\n",
    "london=['london']\n",
    "southeast=['oxford','portsmouth','southampton','reading','brighton','milton keynes','slough','southend-on-sea','luton']\n",
    "southwest=['bath','bristol','plymouth','gloucester','bournemouth','poole']\n",
    "northwest=['manchester','liverpool','lancaster','warrington','bolton','blackpool']\n",
    "northeast=['newcastle','durham','sunderland']\n",
    "yorkshire=['york','leeds','bradford','kingston','sheffield','huddersfield','middlesbrough']\n",
    "west_midland=['birmingham','coventry','stoke-on-trent','wolverhampton','swindon','wolverhampton','telford']\n",
    "east_midland=['derby','nottingham','leicester','northampton','cambridge','peterborough']\n",
    "scotland=['edinburgh','glasgow','aberdeen','dundee']\n",
    "wales=['cardiff','swansea']\n",
    "northern_ireland=['belfast']\n",
    "new_location=[]\n",
    "for location in raw_data.location:\n",
    "    if type(location)==float:\n",
    "        location='unknown'\n",
    "    else:\n",
    "        location=location.split(',')[0].lower()\n",
    "    if location=='uk' or location=='united kingdom':\n",
    "        location='unknown'\n",
    "    elif any(city in location for city in london):\n",
    "        location='london'\n",
    "    elif any(city in location for city in southeast):\n",
    "        location='southeast'\n",
    "    elif any(city in location for city in southwest):\n",
    "        location='southwest'\n",
    "    elif any(city in location for city in northwest):\n",
    "        location='northwest'\n",
    "    elif any(city in location for city in northeast):\n",
    "        location='northeast'\n",
    "    elif any(city in location for city in west_midland):\n",
    "        location='west midland'\n",
    "    elif any(city in location for city in east_midland):\n",
    "        location='east midland'\n",
    "    elif any(city in location for city in yorkshire):\n",
    "        location='yorkshire'\n",
    "    elif any(city in location for city in scotland):\n",
    "        location='scotland'\n",
    "    elif any(city in location for city in wales):\n",
    "        location='wales'\n",
    "    elif any(city in location for city in northern_ireland):\n",
    "        location='northern ireland'\n",
    "    else:\n",
    "        location='other towns and places'\n",
    "    new_location.append(location)\n",
    "raw_data.insert(loc=5, column='new_location', value=new_location)\n",
    "    \n",
    "#Remove all the contact signs and people names mentioned inside the text\n",
    "# Too many emoji with the same content but I can't remove them and they will pollute the word source\n",
    "def deleteRepetition(emoji_content, max_times=2):\n",
    "    emoji=list(emoji_content)\n",
    "    emotion=' '.join(emoji_content)\n",
    "    checklist = [lab for lab in dict.fromkeys(emoji) if emoji.count(lab) > 1]\n",
    "    for i in range(len(checklist)):\n",
    "       while(emoji.count(checklist[i]) > max_times):\n",
    "           emoji.remove(checklist[i])\n",
    "           emotion=''.join(emoji)\n",
    "    return emotion\n",
    "\n",
    "def removeDuplicateEmoji(text):\n",
    "    pure_text=emojiFreeText(text)\n",
    "    duplicate_emoji = []\n",
    "    for emo in text:\n",
    "        if emo in (emoji.UNICODE_EMOJI['en'] or emoji.UNICODE_EMOJI['es'] or emoji.UNICODE_EMOJI['pt'] or emoji.UNICODE_EMOJI['it']):\n",
    "            duplicate_emoji.append(emo)\n",
    "    twice_maximum=deleteRepetition(duplicate_emoji)\n",
    "    text=pure_text+twice_maximum\n",
    "    return text\n",
    "\n",
    "def freeContact(text):\n",
    "    pattern = re.compile(\"@+\")\n",
    "    if len(re.findall(pattern,text))!=0:\n",
    "        pattern = re.compile(\"@+\")\n",
    "        times=len([match.span() for match in pattern.finditer(text)])\n",
    "        time=0\n",
    "        while time<times:\n",
    "            poistion=[match.start() for match in pattern.finditer(text)][0]\n",
    "            text=text[:poistion]+text[poistion+len(text[poistion:].split()[0]):]\n",
    "            text=text.replace(\"  \", \" \")\n",
    "            time+=1\n",
    "        else:\n",
    "            text=text\n",
    "    return text\n",
    "\n",
    "#Restore all emoji into readable meanings\n",
    "def emojiLemon(text):\n",
    "    return emoji.demojize(text, delimiters=(\"\", \" \")) \n",
    "\n",
    "def singleQuoteTransfer(text):\n",
    "    text=re.sub(\"  \",\" \",text)\n",
    "    text=re.sub(\"'ve\",\" have\",text)\n",
    "    text=re.sub(\"'d\",\" would like to\",text)\n",
    "    text=re.sub(\"n't\",\" not\",text)\n",
    "    text=re.sub(\"'m\",\" am\",text)\n",
    "    text=re.sub(\"'s\",\" is\",text)\n",
    "    text=re.sub(\"_\",\" \",text)\n",
    "    text=re.sub('hrs','hours', text)\n",
    "    text=re.sub('pls','please', text)\n",
    "    text=re.sub('bf','boyfriend',text)\n",
    "    text=re.sub('dcpc','driver certificate of professional competence', text)\n",
    "    text=re.sub('celebs','celebrities', text)\n",
    "    return text\n",
    "\n",
    "#remove all the tags used in sentences\n",
    "def removeTag (text):\n",
    "    text=' '.join([letter for letter in text.split() if not len(letter)>=15])\n",
    "    text=text.lower()\n",
    "    text=re.sub('n not',' not',text)\n",
    "    text=re.sub('ca not','can not',text)\n",
    "    return text\n",
    "\n",
    "#remove all punctuation used in sentences\n",
    "def removePunc(text):\n",
    "    text = re.sub(r'''\\W+\\s*''',' ',text,flags=re.VERBOSE)\n",
    "    text=''.join([let for let in text if not let.isdigit()])\n",
    "    return text\n",
    "\n",
    "#remove all two-letter words and some meaningless abbreviations in sentences\n",
    "def meaninglessWordRemove(text):\n",
    "    new_sw=sw.words('english')\n",
    "    new_sw=list(set(new_sw+list(string.ascii_lowercase)))+['much','many','lol','etc','yeah','ummm','haha','omg','hmmm']\n",
    "    new_sw.remove('not')\n",
    "    new_text=[]\n",
    "    for word in text.split():\n",
    "        if len(word)>2:\n",
    "            new_text.append(word)\n",
    "    text=' '.join([word for word in new_text if word not in new_sw])\n",
    "    return text\n",
    "\n",
    "def sentenceLemon(text):\n",
    "        lemon = wn()\n",
    "        each_lemmon_str = ' '.join([lemon.lemmatize(word) for word in tk(text)])\n",
    "        allow_postags = set(['NOUN', 'VERB', 'ADJ', 'ADV', 'PROPN'])\n",
    "        words = []\n",
    "        for token in nlp(each_lemmon_str):\n",
    "            if token.pos_ in allow_postags:\n",
    "                words.append(token.lemma_)\n",
    "        return ' '.join(words)\n",
    "\n",
    "def abbrChange(text):\n",
    "    cos='cos'\n",
    "    bcos='bcos'\n",
    "    rd='rd'\n",
    "    mon='mon'\n",
    "    tue='tues'\n",
    "    wed='wed'\n",
    "    thu='thu'\n",
    "    fri='fri'\n",
    "    sat='sat'\n",
    "    tfl='tfl'\n",
    "    text=re.compile(r'\\b%s\\b' % cos, re.I).sub('because',text)\n",
    "    text=re.compile(r'\\b%s\\b' % bcos, re.I).sub('because',text)\n",
    "    text=re.compile(r'\\b%s\\b' % rd, re.I).sub('road',text)\n",
    "    text=re.compile(r'\\b%s\\b' % mon, re.I).sub('monday',text)\n",
    "    text=re.compile(r'\\b%s\\b' % tue, re.I).sub('tuesday',text)\n",
    "    text=re.compile(r'\\b%s\\b' % wed, re.I).sub('wednesday',text)\n",
    "    text=re.compile(r'\\b%s\\b' % thu, re.I).sub('thursday',text)\n",
    "    text=re.compile(r'\\b%s\\b' % fri, re.I).sub('friday',text)\n",
    "    text=re.compile(r'\\b%s\\b' % sat, re.I).sub('saturday',text)\n",
    "    text=re.compile(r'\\b%s\\b' % tfl, re.I).sub('transport london',text) \n",
    "    return text\n",
    "\n",
    "# Process the original text by sequence of functions\n",
    "def dataProcessingAuto(text):\n",
    "    processed_text=sentenceLemon(\n",
    "        abbrChange(\n",
    "            meaninglessWordRemove(\n",
    "                removeTag(\n",
    "                    removePunc(\n",
    "                        singleQuoteTransfer(\n",
    "                            emojiLemon(\n",
    "                                freeContact(\n",
    "                                    removeDuplicateEmoji(text)))))))))\n",
    "    return processed_text\n",
    "\n",
    "raw_data['new_processed_text']=raw_data['text'].apply(dataProcessingAuto)\n",
    "print('The capacity of the dataset is:',len(raw_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data=pd.read_csv('C:/Users/ky002/Desktop/Dickens/Postgraduate/Dissertation/Data Source/pre_processed_data.csv',sep=',',header=0,encoding='utf_8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Textblob Model\n",
    "def textblobAnalysis(sentiment_text):\n",
    "    blob_attitude = TextBlob(sentiment_text).sentiment.polarity\n",
    "    return blob_attitude\n",
    "    \n",
    "avg_pos_list=[]\n",
    "avg_neg_list=[]\n",
    "\n",
    "for text in raw_data['new_processed_text']:\n",
    "    blob_attitude=textblobAnalysis(text)\n",
    "    if blob_attitude>0.05:\n",
    "        avg_pos_list.append(blob_attitude)\n",
    "    elif blob_attitude<-0.05:\n",
    "        avg_neg_list.append(blob_attitude)\n",
    "\n",
    "avg_pos=sum(avg_pos_list)/len(avg_pos_list)\n",
    "avg_neg=sum(avg_neg_list)/len(avg_neg_list)\n",
    "\n",
    "textblob_attitude=[]\n",
    "for text in raw_data['new_processed_text']:\n",
    "    blob_attitude=textblobAnalysis(text)\n",
    "    attitude=None\n",
    "    if blob_attitude>0.05 and blob_attitude>avg_pos:\n",
    "        attitude='Extremely Positive'\n",
    "    elif blob_attitude>0.05 and blob_attitude<=avg_pos:\n",
    "        attitude='Slightly Positive'\n",
    "    elif abs(blob_attitude)<=0.05:\n",
    "        attitude='Neutral'    \n",
    "    elif blob_attitude<-0.05 and blob_attitude<avg_neg:\n",
    "        attitude='Extremely Negative'\n",
    "    elif blob_attitude<-0.05 and blob_attitude>=avg_neg:\n",
    "        attitude='Slightly Negative'\n",
    "    textblob_attitude.append(attitude)\n",
    "\n",
    "raw_data.insert(loc=10, column='textblob_sentiment', value=textblob_attitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK sentimental Analysis Model add the weigh of SentiWordnet Model\n",
    "def vadorSentiwordnetAnalysisAbsolute(sentiment_text):\n",
    "    posit=[]\n",
    "    negat=[]\n",
    "    length_of_sentence=len(sentiment_text)\n",
    "    range_of_sentence=range(length_of_sentence)\n",
    "    for word in sentiment_text.split(): \n",
    "        each_sentence = swn.senti_synsets(word)\n",
    "        each_sentence_list=list(each_sentence)\n",
    "        if len(each_sentence_list)!=0:\n",
    "            pos=each_sentence_list[0].pos_score()\n",
    "            neg=each_sentence_list[0].neg_score()\n",
    "            posit.append(pos)\n",
    "            negat.append(neg)\n",
    "        else:\n",
    "            pos=0\n",
    "            neg=0\n",
    "            posit.append(pos)\n",
    "            negat.append(neg)\n",
    "        \n",
    "    pos=sum(posit)/length_of_sentence\n",
    "    neg=sum(negat)/length_of_sentence\n",
    "\n",
    "    score=SentimentIntensityAnalyzer().polarity_scores(sentiment_text)\n",
    "    vador_pos=score['pos']\n",
    "    vador_neg=score['neg']\n",
    "\n",
    "    #weigh the negative words more in the sentence\n",
    "    pos=pos+vador_pos\n",
    "    neg=neg+vador_neg*1.5\n",
    "    absolute=abs(pos-neg)\n",
    "    return absolute\n",
    "\n",
    "avg_absolute_list=[]\n",
    "for text in raw_data['new_processed_text'][0:100]:\n",
    "    avg_absolute_list.append(vadorSentiwordnetAnalysisAbsolute(text))\n",
    "\n",
    "avg_absolute=sum(avg_absolute_list)/len(avg_absolute_list)\n",
    "\n",
    "def vadorSentiwordnetAnalysis(sentiment_text):\n",
    "    posit=[]\n",
    "    negat=[]\n",
    "    length_of_sentence=len(sentiment_text)\n",
    "    range_of_sentence=range(length_of_sentence)\n",
    "    for word in sentiment_text.split(): \n",
    "        each_sentence = swn.senti_synsets(word)\n",
    "        each_sentence_list=list(each_sentence)\n",
    "        if len(each_sentence_list)!=0:\n",
    "            pos=each_sentence_list[0].pos_score()\n",
    "            neg=each_sentence_list[0].neg_score()\n",
    "            posit.append(pos)\n",
    "            negat.append(neg)\n",
    "        else:\n",
    "            pos=0\n",
    "            neg=0\n",
    "            posit.append(pos)\n",
    "            negat.append(neg)\n",
    "        \n",
    "    pos=sum(posit)/length_of_sentence\n",
    "    neg=sum(negat)/length_of_sentence\n",
    "\n",
    "    score=SentimentIntensityAnalyzer().polarity_scores(sentiment_text)\n",
    "    vador_pos=score['pos']\n",
    "    vador_neg=score['neg']\n",
    "\n",
    "    #weigh the negative words more in the sentence\n",
    "    pos=pos+vador_pos\n",
    "    neg=neg+vador_neg*1.5\n",
    "\n",
    "    threhold_1=avg_absolute\n",
    "    threhold_2=0.025\n",
    "    \n",
    "    attitude='Slightly Negative'\n",
    "    if (pos-neg)>threhold_1:\n",
    "        attitude='Extremely Positive'\n",
    "    elif pos>neg and (pos-neg)<=threhold_1 and (pos-neg)>threhold_2:\n",
    "        attitude='Slightly Positive'\n",
    "    elif (abs(pos-neg))<=threhold_2:\n",
    "        attitude='Neutral'\n",
    "    elif neg>pos and (neg-pos)>threhold_1:\n",
    "        attitude='Extremely Negative'\n",
    "    elif neg>pos and (neg-pos)<=threhold_1 and (neg-pos)>threhold_2:\n",
    "        attitude='Slightly Negative'\n",
    "    return attitude\n",
    "\n",
    "raw_data['vador_sentiwordnet_sentiment']=raw_data['new_processed_text'].apply(vadorSentiwordnetAnalysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_nearest_words(data, dim):\n",
    "#     n_grams = ng(tk(data), dim)\n",
    "#     return [ ' '.join(grams) for grams in n_grams]\n",
    "\n",
    "# double_title_list=extract_nearest_words(' '.join(sentences),3)\n",
    "# double_title_analysis=Counter(double_title_list).most_common(25)\n",
    "# print(double_title_analysis)\n",
    "\n",
    "# # double_abstract_list=extract_nearest_words(str.join(pre_processing_abstract),2)\n",
    "# # double_abstract_analysis=Counter(double_abstract_list).most_common(25)\n",
    "# # print(double_abstract_analysis)\n",
    "\n",
    "# # most_25_title_common_words=[word for word, word_count in double_title_analysis]\n",
    "# # most_25_abstract_common_words=[word for word, word_count in double_abstract_analysis]\n",
    "\n",
    "# # # the similarity of two selected lists\n",
    "# # similar_words=[word for word in most_25_title_common_words if word in most_25_abstract_common_words]\n",
    "# # print('similar words :', similar_words)\n",
    "# # print('similarity ratio :', len(similar_words)/len(most_25_title_common_words))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "46c53dfe207e1bf458957234df624eec1401b1cc104da5f2e9e6134dc26f11c7"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
